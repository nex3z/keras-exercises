{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "11h3KNqbtZRB",
    "outputId": "1aba52b3-eeb9-4675-a3f8-2c388da67a6e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "VEAyJE48teg1",
    "outputId": "1e01bd3c-36d5-4ffa-e0ad-6750df26fdba"
   },
   "outputs": [],
   "source": [
    "path = keras.utils.get_file(\"nietzsche.txt\", origin=\"https://s3.amazonaws.com/text-datasets/nietzsche.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(path, 'r') as f:\n",
    "#     text = f.read().lower()\n",
    "# print(\"len(text) = {}\".format(len(text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(text) = 600893\n"
     ]
    }
   ],
   "source": [
    "import codecs\n",
    "\n",
    "with codecs.open(path, 'r', encoding='utf-8') as f:\n",
    "    text = f.read().lower()\n",
    "print(\"len(text) = {}\".format(len(text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(charset) = 57\n",
      "Unique chars = ['\\n', ' ', '!', '\"', \"'\", '(', ')', ',', '-', '.', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '=', '?', '[', ']', '_', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'ä', 'æ', 'é', 'ë']\n"
     ]
    }
   ],
   "source": [
    "charset = sorted(list(set(text)))\n",
    "print(\"len(charset) = {}\".format(len(charset)))\n",
    "print(\"Unique chars = {}\".format(charset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WAo_BPbYtiZk"
   },
   "outputs": [],
   "source": [
    "MAX_LEN = 60\n",
    "STEP = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "A_8gawNFtkUS"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(sentences) = 200278\n"
     ]
    }
   ],
   "source": [
    "sentences = []\n",
    "next_chars = []\n",
    "\n",
    "for i in range(0, len(text) - MAX_LEN, STEP):\n",
    "    sentences.append(text[i: i + MAX_LEN])\n",
    "    next_chars.append(text[i + MAX_LEN])\n",
    "    \n",
    "print(\"len(sentences) = {}\".format(len(sentences)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "WV-BDgyFuQaD",
    "outputId": "8410b487-592f-422e-9eb1-f433f116dac0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'preface\\n\\n\\nsupposing that truth is a woman--what then? is there not ground\\nfor su'\n",
      "'preface\\n\\n\\nsupposing that truth is a woman--what then? is the'\n",
      "'r'\n"
     ]
    }
   ],
   "source": [
    "print(repr(text[:MAX_LEN+20]))\n",
    "print(repr(sentences[0]))\n",
    "print(repr(next_chars[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7ZfZOxYUbTP2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "char_idx_dict = {'\\n': 0, ' ': 1, '!': 2, '\"': 3, \"'\": 4, '(': 5, ')': 6, ',': 7, '-': 8, '.': 9, '0': 10, '1': 11, '2': 12, '3': 13, '4': 14, '5': 15, '6': 16, '7': 17, '8': 18, '9': 19, ':': 20, ';': 21, '=': 22, '?': 23, '[': 24, ']': 25, '_': 26, 'a': 27, 'b': 28, 'c': 29, 'd': 30, 'e': 31, 'f': 32, 'g': 33, 'h': 34, 'i': 35, 'j': 36, 'k': 37, 'l': 38, 'm': 39, 'n': 40, 'o': 41, 'p': 42, 'q': 43, 'r': 44, 's': 45, 't': 46, 'u': 47, 'v': 48, 'w': 49, 'x': 50, 'y': 51, 'z': 52, 'ä': 53, 'æ': 54, 'é': 55, 'ë': 56}\n"
     ]
    }
   ],
   "source": [
    "char_idx_dict = dict((char, charset.index(char)) for char in charset)\n",
    "print(\"char_idx_dict = {}\".format(char_idx_dict))\n",
    "\n",
    "def encode_sentence(sentences):\n",
    "    if not isinstance(sentences, list):\n",
    "        sentences = [sentences]\n",
    "    \n",
    "    encoded = np.zeros((len(sentences), MAX_LEN, len(charset)))\n",
    "    for i, sentence in enumerate(sentences):\n",
    "        for t, char in enumerate(sentence):\n",
    "            encoded[i, t, char_idx_dict[char]] = 1\n",
    "    return encoded\n",
    "\n",
    "def encode_char(chars):\n",
    "    if not isinstance(chars, list):\n",
    "        chars = [chars]\n",
    "    \n",
    "    encoded = np.zeros((len(chars), len(charset)))\n",
    "    for i, char in enumerate(chars):\n",
    "        encoded[i, char_idx_dict[char]] = 1\n",
    "    return encoded\n",
    "\n",
    "def decode_sentence(one_hot_sentence):\n",
    "    chars = [charset[i] for i in one_hot_sentence.argmax(axis=1)]\n",
    "    return ''.join(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nSOQBfIDay1g"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x.shape = (200278, 60, 57)\n",
      "y.shape = (200278, 57)\n"
     ]
    }
   ],
   "source": [
    "x = encode_sentence(sentences)\n",
    "y = encode_char(next_chars)\n",
    "\n",
    "print(\"x.shape = {}\".format(x.shape))\n",
    "print(\"y.shape = {}\".format(y.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'preface\\n\\n\\nsupposing that truth is a woman--what then? is the'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decode_sentence(x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 105
    },
    "colab_type": "code",
    "id": "HlPxOtYBw1x9",
    "outputId": "0458683f-e881-429f-996c-86ed5f25583e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From d:\\Dev\\Miniconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(128, input_shape=(MAX_LEN, len(charset))))\n",
    "model.add(Dense(len(charset), activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RuB3VLRkyE_5"
   },
   "outputs": [],
   "source": [
    "LEARNING_RATE = 0.01\n",
    "BATCH_SIZE = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0xb197apxSur"
   },
   "outputs": [],
   "source": [
    "from keras.optimizers import RMSprop\n",
    "\n",
    "optimizer = RMSprop(lr=LEARNING_RATE)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From d:\\Dev\\Miniconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/1\n",
      "200278/200278 [==============================] - 161s 805us/step - loss: 1.9646\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x16071f9e358>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x, y, batch_size=BATCH_SIZE, epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6WMHKqlR3k5k"
   },
   "outputs": [],
   "source": [
    "def sample(preds, temperature=1.0):\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "g4A-K8w2PR6P"
   },
   "outputs": [],
   "source": [
    "def encode_one_hot(text):\n",
    "    encoded = np.zeros((1, MAX_LEN, len(text)))\n",
    "    for idx, c in enumerate(text):\n",
    "        encoded[0, idx, char_idx_dict[c]] = 1\n",
    "    return encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting with: 'will do.--what is lacking in\\nengland, and has always been la'\n",
      "======= temperature: 0.2 =======\n",
      "will do.--what is lacking in\n",
      "england, and has always been lat such as a pertical superination of the from the sense to been they will they they they they they in the sense to the most be not to in they not they in the superinations and to they to they and the great such they will they they they himself they they they such a most present they they to his some they they they in the superination of the deners and they to be the inselfise to the forment of the\n",
      "======= temperature: 0.5 =======\n",
      "he deners and they to be the inselfise to the forment of theyself supernantions they they they in nothing more how the don they he oun all their our with must they explaness.\n",
      "\n",
      "\n",
      "\n",
      "1chulled to the present to meath is new man and more they insompations to in ording and consections, they we\n",
      "supponing there and all to distoul and a mistances in lature and some all they wimh the father to they haging to a manter and we mand to contly himself in the god anding and\n",
      "======= temperature: 1.0 =======\n",
      "a manter and we mand to contly himself in the god anding and :cauluring. of fas of their manedly undentess to fint, as take. in man\"- to paternth, in hootherin, they, he him,\n",
      "tineble superitual true with reworcy which human in who\n",
      "every tamred makescimad futunce things have diffifoodinntand woth his ade, good they great bring its youn spreiticnly. theeeself an inseeming in pates then takenish\n",
      "pains impulsts tuonin valenct of bad it seecl of knows \"touring \n",
      "======= temperature: 1.2 =======\n",
      "s impulsts tuonin valenct of bad it seecl of knows \"touring in reglear: estimr with uuwhow wither\n",
      "supversious, if in\n",
      "eulpearal smating is expepacibanes--prithenigatod exsepfutife, pleamentaw.\n",
      "s\n",
      "mac are hat-fisenh utwarpble and howe\" science outherthingsolin., owtelves insundeastain himself to the peadeparing who\n",
      "findlin\n",
      "masegan sciengion\n",
      "wifithe. in (povernt so pinw\"- i men as therefored in all rims prefined or\n",
      "afford ougr sen of aboutims\" they to-fance su\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import sys\n",
    "\n",
    "start_index = random.randint(0, len(text) - MAX_LEN - 1)\n",
    "generated_text = text[start_index:(start_index + MAX_LEN)]\n",
    "print(\"Starting with: {}\".format(repr(generated_text)))\n",
    "\n",
    "for temperature in [0.2, 0.5, 1.0, 1.2]:\n",
    "    print(\"======= temperature: {} =======\".format(temperature))\n",
    "    sys.stdout.write(generated_text)\n",
    "    for i in range(400):\n",
    "        sampled = encode_sentence(generated_text)\n",
    "        preds = model.predict(sampled, verbose=0)[0]\n",
    "        next_index = sample(preds, temperature)\n",
    "        next_char = charset[next_index]\n",
    "        sys.stdout.write(next_char)\n",
    "        sys.stdout.flush()\n",
    "        generated_text += next_char\n",
    "        generated_text = generated_text[1:]\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "text-generation-with-lstm.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
